pretrained_path: /home/jayll/VoxCPM/pretrained/VoxCPM-1.5B
train_manifest: /mnt/d/tts_dataset/merged_metadata_with_duration.jsonl
val_manifest: null
sample_rate: 44100
batch_size: 4
grad_accum_steps: 2  # Gradient accumulation steps, >1 can increase effective batch size without increasing memory
num_workers: 16
num_iters: 100000
log_interval: 10
valid_interval: 1000
save_interval: 1000
learning_rate: 0.00001 
weight_decay: 0.01
warmup_steps: 100
max_steps: 100000
max_batch_tokens: 8192  # Example: single batch can have at most 16k tokens, with batch_size=4, each sample can have at most 4096 tokens
save_path: /home/jayll/VoxCPM/checkpoints/1.5Bv2
tensorboard: /home/jayll/VoxCPM/logs/1.5Bv2
lambdas:
  loss/diff: 1.0
  loss/stop: 1.0
